{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 03\n",
    "\n",
    "**Due:** Thursday, 2018-03-01, 11:59 PM, as a Jupyter notebook submitted via your repo in the course GitHub organization.  Edit the provided Solutions03 notebook with your solutions.  All  subproblems (or problems with no subproblems) are worth 1 point unless otherwise noted.\n",
    "\n",
    "In derivations, use a bit of text to explain your steps, but you needn't write an essay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The \"Or\" rule\n",
    "\n",
    "Recall the disjunction (logical \"or\") rule:\n",
    "$$\n",
    "P(A\\lor B) = P(A) + P(B) - P(A\\land B) \\qquad ||\\;\\mathcal{C}.\n",
    "$$\n",
    "In the majority of applications, the set of propositions of interest (about either parameters or data) typically comprise an *exhaustive, mutually exclusive* set.  (*Think Bayes* author Allen Downey uses the term *suite* to denote such a set of propositions; I like that!)  For mutually exclusive propositions, $P(A\\land B) = 0$. For example, if a parameter has one value, it can't simultaneously be another value; similarly for the value of a datum. In this case, the \"or\" rule simplifies (dropping the $\\mathcal{C}$ henceforth):\n",
    "$$\n",
    "P(A\\lor B) = P(A) + P(B).\n",
    "$$\n",
    "It's simpler than the full \"or\" rule, though we've only dropped 1 term.\n",
    "\n",
    "When we derived the LTP, we used this simpler \"or\" rule, but extended to an arbitrary number of propositions, $B_i$, comprising a suite. The purpose of this exercise is to help you appreciate the impact of working with a mutually exclusive set of alternatives.  It simplifies things more than you may have realized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (2 points):\n",
    "\n",
    "> Use the \"or\" and \"and\" rules to find an expression for $P(A\\lor B \\lor C)$, *not* assuming that $A$, $B$, and $C$ are mutually exclusive. Seek a result using only probabilities for the individual propositions, and \"and\" combinations of them. (You can use a comma instead of $\\land$ for \"and\" if you wish.)  You may use associativity of \"or\" and \"and;\" that is, $A\\lor (B \\lor C) \\equiv (A\\lor B) \\lor C$, and $A\\land (B \\land C) \\equiv (A\\land B) \\land C$.\n",
    "\n",
    "> Do this in two steps:\n",
    "\n",
    "> * Copy the following incomplete truth tables into your solution cell and complete them. For each table, you should find that the two bold columns have the same truth values, indicating they are logically equivalent. These tables thus establish two *rules of replacement*: where you see one of these formulas, you may substitute the other.  MathJax will probably make the headings hard to read, so to clarify: the first table aims to show $X\\land Y\\land X \\equiv X\\land Y$ (so you can drop repeated symbols in a multiple \"and\"), and the second table aims to show a kind of distributive rule: $X\\land (Y\\lor Z) \\equiv (X\\land Y)\\lor (X\\land Z)$.\n",
    "\n",
    "> *Hint*: You may find it easier to work on the tables using the [Markdown Tables generator - TablesGenerator.com](https://www.tablesgenerator.com/markdown_tables) web page. Note that you can copy the Markdown for a table to your clipboard, and load it into the web page using the `File->Paste` command on the web page.\n",
    "\n",
    "| $X$ | $Y$ | ${\\bf X\\land Y}$ | ${\\bf (X\\land Y) \\land X}$ |\n",
    "|---|-----|------------|----------------------|\n",
    "| 0   | 0   |            |                      |\n",
    "| 0   | 1   |            |                      |\n",
    "| 1   | 0   |            |                      |\n",
    "| 1   | 1   |            |                      |\n",
    "\n",
    "\n",
    "| $X$ | $Y$ | $Z$ | $Y\\lor Z$ | ${\\bf X\\land (Y\\lor Z)}$  | $X\\land Y$ | $X\\land Z$ | ${\\bf (X\\land Y)\\lor (X\\land Z)}$ |\n",
    "|---|-----|-----|-----------|---------------------|------------|------------|-----------------------------|\n",
    "| 0   | 0   | 0   |           |                     |            |            |                             |\n",
    "| 0   | 0   | 1   |           |                     |            |            |                             |\n",
    "| 0   | 1   | 0   |           |                     |            |            |                             |\n",
    "| 0   | 1   | 1   |           |                     |            |            |                             |\n",
    "| 1   | 0   | 0   |           |                     |            |            |                             |\n",
    "| 1   | 0   | 1   |           |                     |            |            |                             |\n",
    "| 1   | 1   | 0   |           |                     |            |            |                             |\n",
    "| 1   | 1   | 1   |           |                     |            |            |                             |\n",
    "\n",
    "> * With those replacement rules in hand, proceed with deriving the 3-proposition \"or\" rule.\n",
    "\n",
    "> *Hint*: You might guess from the 2-proposition \"or\" rule that the answer is $P(A\\lor B \\lor C) = P(A) + P(B) + P(C) - P(A\\land B \\land C)$. But that's not right. It's quite a bit more complicated (which is why having mutually exclusive alternatives is a great help)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discovery chains\n",
    "\n",
    "The parameter estimation problems addressed in lectures 5 through 7 involved the use of *conjugate families*: prior/likelihood pairs such that if the prior is a distribution in a particular family, and the likelihood function is built from a sampling distribution in the partner family, then the posterior distribution is another distribution from the prior's family. A handy feature of conjugate families is that inferences can be easily chained, with the posterior from inference based on an initially considered dataset, $D_1$, serving as the prior for analysis of a subsequently considered dataset, $D_2$, and producing an updated posterior that can be found by simple manipulation of the parameters in the prior family.\n",
    "\n",
    "For example, in Lec05 we considered inference of a binary outcome probability, $\\alpha$, with binomial data comprising $n$ successes in $N$ trials.  We found the posterior PDF for $\\alpha$, based on a beta PDF prior with parameters $(a,b)$:\n",
    "\\begin{align}\n",
    "p(\\alpha|n,M') \n",
    "  &\\propto \\mathop{\\mathrm{Beta}}(\\alpha|a,b) \\times \\mathop{\\mathrm{Binom}}(n|\\alpha,N)\\\\\n",
    "  &\\propto \\alpha^{a-1} (1-\\alpha)^{b-1} \\times \\alpha^{n} (1-\\alpha)^{N-n}\\\\\n",
    "  &\\propto \\alpha^{n+a-1} (1-\\alpha)^{N-n+b-1}.\n",
    "\\end{align}\n",
    "That is, the posterior is $\\mathop{\\mathrm{Beta}}(\\alpha|n+a,N-n+b)$. So schematically, the update rule, telling us how a beta prior is modified into a beta posterior, is:\n",
    "$$\n",
    "(a,\\; b) \\quad\\Rightarrow\\quad (n+a,\\; N-n+b).\n",
    "$$\n",
    "Thus if we get additional data composed of $n'$ successes in $N'$ new trials, the final posterior PDF will again be a beta PDF, with parameters found by chaining the update rule:\n",
    "$$\n",
    "(a,\\; b) \\quad\\Rightarrow\\quad (n+a,\\; N-n+b) \\quad\\Rightarrow\\quad (n+n'+a,\\; N+N'-n-n'+b).\n",
    "$$\n",
    "\n",
    "A simple but interesting consequence of this update rule is that we get the same overall posterior if we *pool the data*, i.e., consider a *single* binomial dataset composed of $m=n+n'$ successes in $M=N+N'$ trials.  Applying the single-stage update rule to this pooled data gives\n",
    "$$\n",
    "(a,\\; b) \\quad\\Rightarrow\\quad (m+a,\\; M-m+b) \\quad=\\quad (n+n'+a,\\; N+N'-n-n'+b),\n",
    "$$\n",
    "the same result as we found from chaining the inferences.\n",
    "\n",
    "This result is nearly trivial because, in the binomial sampling setting, the probability for the 2nd dataset is independent of the outcome comprising the 1st dataset, if we are given $\\alpha$ (as we are in a likelihood function). Independence implies that the likelihood function based on the pooled data is just the product of the likelihood functions considering each dataset separately, and consistency follows easily from this.\n",
    "\n",
    "In fact, the consistency of the chained and pooled inferences is a more general result that *does not require the two datasets to have sampling distributions that are conditionally independent* (i.e., independent given the parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:\n",
    "\n",
    "> Prove the consistency of chained and joint inferences based on using two datasets to estimate a parameter, $\\theta$.\n",
    "\n",
    "> 1. Use Bayes's theorem to write down the posterior PDF for $\\theta$ based on data $D_1$.\n",
    "> 2. Use the posterior from (1) as the prior for inference of $\\theta$ additionally considering new data, $D_2$, using Bayes's theorem to compute an overall posterior PDF for $\\theta$, $p(\\theta|D_1,D_2,\\mathcal{C})$. *Do not assume that the joint sampling distribution for $(D_1,D_2)$ factors*:\n",
    "$$\n",
    "p(D_1,D_2|\\theta) \\ne p(D_1|\\theta)\\times p(D_2|\\theta). \\qquad ||\\; \\mathcal{C}\n",
    "$$\n",
    "> 3. Now suppose you start with the same initial prior used in (1), but consider the two datasets together. Compute the posterior $p(\\theta|D_1,D_2,\\mathcal{C})$ in a single step, considering $(D_1,D_2)$ as a single, pooled dataset.\n",
    "> 4. Show that the results of (2) and (3) are equal.\n",
    "\n",
    "> For convenience, you may drop $\\mathcal{C}$ from the notation, since the same contextual information is being used throughout.  *Hint:* You shouldn't have to write out any marginal likelihoods (e.g., in terms of integrals of prior times likelihood) in order to prove consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dirichlet distributions and aggregation\n",
    "\n",
    "In Lab04, we reconsidered inference of categorical or multinomial probabilities, $\\alpha_k$ (for $k=1$ to $K$), as covered in Lec06. We used simulation from the prior (i.e., drawing samples from the prior PDF) to show that a uniform (flat, or constant) prior for the $\\alpha_k$ parameters allows for diverse categorical distributions when $K$ is small, but concentrates increasingly strongly around $\\alpha_k = 1/K$ as $K$ becomes large. As a result, for inference with many categories, even for large sample sizes, we could infer with high probability that all categories are nearly equally likely, even if all outcomes are only in a few categories. This highlights that our intuition of what it may mean to be \"uninformative\" in low dimensions can be faulty when applied to problems with high dimensionality.\n",
    "\n",
    "As an alternative to the uniform prior, we explored a non-uniform Dirichlet prior with two features:\n",
    "\n",
    "* We focused on the class of *symmetric* Dirichlet distributions, with all concentration parameters equal to the same constant, $\\kappa$. This was motivated by interpreting \"uninformative\" to mean, as a minimum, that all categories should be treated symmetrically, a priori.\n",
    "* Further, we looked at the special case, $\\kappa = C/K$, for some constant $C$.  That is, we took the concentration parameters to decrease as the number of categories increases, inversely proportional to $K$.\n",
    "\n",
    "Note that if we take the constant $C=2$, then for $K=2$, we have $\\kappa = 1$, corresponding to a uniform prior for the 2-category case (the Bernoulli or binomial case of Lec05).  For larger $K$, such a symmetric Dirichlet prior will be *nonuniform*.  Using posterior samples, we found that this nonuniformity allowed for a greater variety of categorical distributions sampled from the prior.  E.g., distributions that allowed for just a few categories to dominate are not strongly ruled out by this prior when $K$ is large (an improvement over the uniform prior behavior).\n",
    "\n",
    "The $\\kappa \\propto 1/K$ dependence appeared out of nowhere in Lab04.  This exercise aims to provide some motivation for that choice, and to give you some practice working with Dirichlet distributions and $\\delta$ functions.\n",
    "\n",
    "For reference, the Dirichlet PDF (using the notation of Lec06) is:\n",
    "$$\n",
    "\\mathop{\\mathrm{Dir}}(\\alpha|\\kappa_1,\\ldots,\\kappa_K)\n",
    "  = \\frac{\\Gamma(\\kappa_0)}{\\Gamma(\\kappa_1)\\cdots\\Gamma(\\kappa_K)}\n",
    "    \\left[\\prod_{k=1}^K \\alpha_k^{\\kappa_k-1}\\right]\n",
    "    \\delta\\left(1-\\sum_{k=1}^K \\alpha_k\\right)\n",
    "$$\n",
    "where $\\kappa_0 = \\sum_{k=1}^K \\kappa_k$.\n",
    "Its normalization constant comes from the *generalized beta integral* (GBI; this is just a mathematical result that needn't refer to probability distributions):\n",
    "\\begin{split}\n",
    "\\int_0^\\infty d\\alpha_1 \\cdots \\int_0^\\infty d\\alpha_K\\;\n",
    "  & \\alpha_1^{\\kappa_1-1}\\cdots\\alpha_K^{\\kappa_K-1} \\delta\\left(a-\\sum_k\\alpha_k\\right)\\\\  % split!\n",
    "  &= \\frac{\\Gamma(\\kappa_1)\\cdots\\Gamma(\\kappa_K)}{\\Gamma(\\kappa_0)}\\; a^{\\kappa_0-1},\n",
    "\\end{split}\n",
    "where $\\kappa_0 = \\sum_{k=1}^K \\kappa_k$. Take note of the $a$ variable appearing in the GBI. To compute the Dirichlet PDF normalization, we simply set $a=1$.  But for some calculations with the Dirichlet, other choices of $a$ may be relevant.\n",
    "\n",
    "A useful special case is $K=2$, where the GBI generalizes the regular, two-category beta integral to cases where the two variables sum to something other than unity:\n",
    "\\begin{align}\n",
    "\\int d\\alpha_1 \\int d\\alpha_2\\;\n",
    "    \\alpha_1^{\\kappa_1-1} \\alpha_2^{\\kappa_2-1} \\delta\\left(a-[\\alpha_1+\\alpha_2]\\right)\n",
    "  &= \\int d\\alpha_1\\; \\alpha_1^{\\kappa_1-1} (a - \\alpha_1)^{\\kappa_2-1}  \\qquad \\mbox{[delta sets $\\alpha_2 = a-\\alpha_1$]}\\\\\n",
    "  &= \\frac{\\Gamma(\\kappa_1) \\Gamma(\\kappa_2)}{\\Gamma(\\kappa_1+\\kappa_2)}\\; a^{\\kappa_1+\\kappa_2-1}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 (3 points):\n",
    "\n",
    "> *Suppose you assign a symmetric Dirichlet PDF to a problem with $K=4$ categories. What are the implications of such a prior if you **aggregate** pairs of categories?*\n",
    "> * Start with the symmetric Dirichlet PDF for $K=4$, with concentration parameter $\\kappa$:\n",
    "$$\n",
    "p(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\;\\propto\\;\n",
    "  \\alpha_1^{\\kappa-1}\\alpha_2^{\\kappa-1}\\alpha_3^{\\kappa-1}\\alpha_4^{\\kappa-1}\\;\n",
    "  \\delta\\left(1-\\sum_k\\alpha_k\\right).\n",
    "$$\n",
    "Note that here we've dropped the gamma functions comprising the normalization constant; you can (and should!) ignore parameter-independent constants throughout this problem (use the MathJax `\\propto` symbol to indicate proportionality).\n",
    "> * Consider the derived parameter $\\beta \\equiv \\alpha_1 + \\alpha_2$, the probability for an outcome being in either category 1 or category 2.  (Note that $1-\\beta = \\alpha_3 + \\alpha_4$, corresponding to aggregating the remaining two categories).  Since $\\beta$ is a function of the $\\alpha$'s, the prior for the $\\alpha$'s implies a prior for $\\beta$.  The prior PDF for $\\beta$ can be found using LTP as follows:\n",
    "\\begin{align}\n",
    "p(\\beta)\n",
    "  &= \\int d\\alpha_1 \\cdots \\int d\\alpha_4\\; p(\\beta, \\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\qquad \\mbox{[extend the conversation]}\\\\\n",
    "  &= \\int d\\alpha_1 \\cdots \\int d\\alpha_4\\; p(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\; \n",
    "     p(\\beta | \\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\qquad \\mbox{[product rule]}\\\\\n",
    "  &= \\int d\\alpha_1 \\cdots \\int d\\alpha_4\\; p(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\; \n",
    "     \\delta(\\beta - [\\alpha_1+\\alpha_2]),\n",
    "\\end{align}\n",
    "where in the last line we used\n",
    "$$\n",
    "p(\\beta | \\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) = \\delta(\\beta - [\\alpha_1+\\alpha_2]).\n",
    "$$\n",
    "That is, if we know the $\\alpha$'s, then we know that $\\beta$ must be exactly equal to $(\\alpha_1 + \\alpha_2)$, which we can enforce with a $\\delta$ function (by construction, a PDF concentrated at a point).\n",
    "\n",
    "> Your task is to do the integral giving $p(\\beta)$ and explore the result, as follows (do each step separately).\n",
    "\n",
    "> 1. Plug the expression for the $\\alpha$ prior into the equation for $p(\\beta)$; the integrand will have *two* $\\delta$ functions in it, one from the $\\alpha$ prior, and one fixing $\\beta$. \n",
    "> 2. Note that the $\\beta$-dependent $\\delta$ function doesn't depend on $(\\alpha_3,\\alpha_4)$. Isolate the parts depending on $(\\alpha_3,\\alpha_4)$, and do the double integral over $\\alpha_3$ and $\\alpha_4$ using the GBI.\n",
    "> 3. Integrals over $\\alpha_1$ and $\\alpha_2$ remain.  Use the remaining $\\delta$ function to do the $\\alpha_2$ integral.\n",
    "> 4. Finally, do the $\\alpha_1$ integral using the GBI, leaving only an expression in terms of $\\beta$ and constants.\n",
    "> 5. You should find that the prior for $\\beta$ is a beta distribution (i.e., a $K=2$ Dirichlet).  What value of $\\kappa$ would make this beta distribution equal to the uniform distribution we used as a prior for inference with binomial data?  Is the original 4-category $\\alpha$ prior corresponding to this $\\kappa$ uniform with respect to $(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The normal-normal conjugate model\n",
    "\n",
    "In Lecture 08 we covered basic inference with the normal (Gaussian) distribution.  Without derivation, I claimed that a normal prior is the conjugate prior for a likelihood function based on data with a normal distribution for the additive noise or error terms.  Verify this claim *numerically*.  The following instructions use the notation in the Lecture 08 slides, for the problem of inferring $\\mu$ with $\\sigma$ considered *known*.\n",
    "\n",
    "For this problem, write a separate script (say, \"NormNorm.py\") implementing your solution.  At the end of your notebook, run the script in the notebook (using IPython's `run` command; make sure you have `ion()` in the notebook or the script).  Then run the `pytest` command-line program in your notebook using \"`!pytest NormNorm.py`\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1:\n",
    "\n",
    "> Calculate and plot the posterior PDF using the analytic formula from lecture:\n",
    "> * Use the scipy.stats.norm distribution object to generate a single sample of $N$ observations, $d_i$, following the model in the lecture.  Pick your own $N$, and your own \"true\" values of the parameters $\\mu$ and $\\sigma$ for the observations.\n",
    "> * Pick a prior mean, $\\mu_0$, and standard deviation, $w_0$, defining a normal prior.  Plot the posterior PDF for $\\mu$ using the formula presented in class for the conjugate posterior (the formula with the quantity $B$ specifying how much the posterior shrinks toward the prior).  Use the numpy `linspace` function to make an array of $\\mu$ values over which you'll evaluate the PDF.  You may use either the scipy.stats `norm` object, or explicit calculation (with `exp`, etc.), to evaluate the PDF.  Use a thick solid curve for the plot (say, with lw=2 or 3 in the matplotlib `plot` function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.2:\n",
    "\n",
    "> Now explicitly calculate and plot the posterior PDF from the prior and likelihood: \n",
    "* Use the same grid of $\\mu$ values used for Problem 4.1.\n",
    "* Evaluate the normal prior and the likelihood function on the grid.\n",
    "* Calculate the prior $\\times$ likelihood, and normalize it using the trapezoid rule (code the trapezoid rule explicitly; don't use `numpy.trapz`).\n",
    "* Plot the resulting normalized PDF on the same axes as Problem 4.1.  Use a dashed line style (and optionally transparency, via the `alpha` argument to `plot`) so that both curves are visible (they should overlap!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.3:\n",
    "\n",
    "> Create test cases that verify elements of your computation:\n",
    "* Create a case that checks whether your trapezoid rule integration matches the result given by `numpy.trapz`.\n",
    "* Create a case that checks whether the two posterior PDFs match over the grid of $\\mu$ values.\n",
    "* Include a `nosetests` run in your notebook that verifies the tests pass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
