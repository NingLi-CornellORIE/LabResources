{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab04 Exercises\n",
    "\n",
    "These exercises will be part of a future assignment. In derivations, use a bit of text to explain your steps, but you needn't write an essay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discovery chains\n",
    "\n",
    "The parameter estimation problems addressed in lectures 5 through 7 involved the use of *conjugate families*: prior/likelihood pairs such that if the prior is a distribution in a particular family, and the likelihood function is built from a sampling distribution in the partner family, then the posterior distribution is another distribution from the prior's family. A handy feature of conjugate families is that inferences can be easily chained, with the posterior from inference based on an initially considered dataset, $D_1$, serving as the prior for analysis of a subsequently considered dataset, $D_2$, and producing an updated posterior that can be found by simple manipulation of the parameters in the prior family.\n",
    "\n",
    "For example, in Lec05 we considered inference of a binary outcome probability, $\\alpha$, with binomial data comprising $n$ successes in $N$ trials.  We found the posterior PDF for $\\alpha$, based on a beta PDF prior with parameters $(a,b)$:\n",
    "\\begin{align}\n",
    "p(\\alpha|n,M') \n",
    "  &\\propto \\mathop{\\mathrm{Beta}}(\\alpha|a,b) \\times \\mathop{\\mathrm{Binom}}(n|\\alpha,N)\\\\\n",
    "  &\\propto \\alpha^{a-1} (1-\\alpha)^{b-1} \\times \\alpha^{n} (1-\\alpha)^{N-n}\\\\\n",
    "  &\\propto \\alpha^{n+a-1} (1-\\alpha)^{N-n+b-1}.\n",
    "\\end{align}\n",
    "That is, the posterior is $\\mathop{\\mathrm{Beta}}(\\alpha|n+a,N-n+b)$. So schematically, the update rule, telling us how the beta prior is modified into a beta posterior, is:\n",
    "$$\n",
    "(a,\\; b) \\quad\\Rightarrow\\quad (n+a,\\; N-n+b).\n",
    "$$\n",
    "Thus if we get additional data composed of $n'$ successes in $N'$ new trials, the final posterior PDF will again be a beta PDF, with parameters found by chaining the update rule:\n",
    "$$\n",
    "(a,\\; b) \\quad\\Rightarrow\\quad (n+a,\\; N-n+b) \\quad\\Rightarrow\\quad (n+n'+a,\\; N+N'-n-n'+b).\n",
    "$$\n",
    "\n",
    "A simple but interesting consequence of this update rule is that we get the same overall posterior if we pool the data, i.e., considering a *single* binomial dataset composed of $m=n+n'$ successes in $M=N+N'$ trials.  Applying the single-stage update rule to this pooled data gives\n",
    "$$\n",
    "(a,\\; b) \\quad\\Rightarrow\\quad (m+a,\\; M-m+b) \\quad=\\quad (n+n'+a,\\; N+N'-n-n'+b),\n",
    "$$\n",
    "the same result as we found from chaining the inferences.\n",
    "\n",
    "This result is nearly trivial because, in the binomial sampling setting, the probability for the 2nd dataset is independent of the outcome comprising the 1st dataset, if we are given $\\alpha$ (as we are in a likelihood function). Independence implies that the likelihood function based on the pooled data is just the product of the likelihood functions considering each dataset separately, and consistency follows easily from this.\n",
    "\n",
    "In fact, the consistency of the chained and pooled inferences is a more general result that *does not require the two datasets to have sampling distributions that are conditionally independent* (i.e., independent given $\\alpha$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:\n",
    "\n",
    "> Prove the consistency of chained and joint inferences based on using two datasets to estimate a parameter, $\\theta$.\n",
    "\n",
    "> 1. Use Bayes's theorem to write down the posterior PDF for $\\theta$ based on data $D_1$.\n",
    "> 2. Use the posterior from (1) as the prior for inference of $\\theta$ additionally considering new data, $D_2$, using Bayes's theorem to compute an overall posterior PDF for $\\theta$, $p(\\theta|D_1,D_2,\\mathcal{C})$. *Do not assume that the joint sampling distribution for $(D_1,D_2)$ factors*:\n",
    "$$\n",
    "p(D_1,D_2|\\theta) \\ne p(D_1|\\theta)\\times p(D_2|\\theta). \\qquad ||\\; \\mathcal{C}\n",
    "$$\n",
    "> 3. Now suppose you start with the same initial prior used in (1), but consider the two datasets together. Compute the posterior $p(\\theta|D_1,D_2,\\mathcal{C})$ in a single step, considering $(D_1,D_2)$ as a single, pooled dataset.\n",
    "> 4. Show that the results of (2) and (3) are equal.\n",
    "\n",
    "> For convenience, you may drop $\\mathcal{C}$ from the notation, since the same contextual information is being used throughout.  *Hint:* You shouldn't have to write out any marginal likelihoods (e.g., in terms of integrals of prior times likelihood) in order to prove consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dirichlet distributions and aggregation\n",
    "\n",
    "In Lab04, we reconsidered inference of categorical or multinomial probabilities, $\\alpha_k$ (for $k=1$ to $K$), as covered in Lec06. We used simulation from the prior (i.e., drawing samples from the prior PDF) to show that a uniform (flat, or constant) prior for the $\\alpha_k$ parameters allows for diverse categorical distributions when $K$ is small, but concentrates increasingly strongly around $\\alpha_k = 1/K$ as $K$ becomes large. As a result, for inference with many categories, even for large sample sizes, we could conclude that all categories are nearly equally likely, even if all outcomes are only in a few categories. This highlights that our intuition of what it may mean to be \"uninformative\" in low dimensions can be faulty when applied to problems with high dimensionality.\n",
    "\n",
    "As an alternative to the uniform prior, we explored a non-uniform Dirichlet prior with two features:\n",
    "\n",
    "* We focused on the class of *symmetric* Dirichlet distributions, with all concentration parameters equal to the same constant, $\\kappa$. This was motivated by interpreting \"uninformative\" to mean, as a minimum, that all categories should be treated equally, a priori.\n",
    "* Further, we looked at the special case, $\\kappa = C/K$, for some constant $C$.  That is, we took the concentration parameters to decrease as the number of categories increases, inversely proportional to $K$.\n",
    "\n",
    "Note that if we take the constant $C=2$, then for $K=2$, we have $\\kappa = 1$, corresponding to a uniform prior for the 2-category case (the Bernoulli or binomial case).  For larger $K$, such a symmetric Dirichlet prior will be *nonuniform*.  Using posterior samples, we found that this nonuniformity allowed for a greater variety of categorical distributions sampled from the prior.  E.g., distributions that allowed for just a few categories to dominate are not strongly ruled out by this prior when $K$ is large.\n",
    "\n",
    "The $\\kappa \\propto 1/K$ dependence appeared out of nowhere in Lab04.  This exercise aims to provide some motivation for that choice, and to give you some practice working with Dirichlet distributions and $\\delta$ functions.\n",
    "\n",
    "For reference, the Dirichlet PDF (using the notation of Lec06) is:\n",
    "$$\n",
    "\\mathop{\\mathrm{Dir}}(\\alpha|\\kappa_1,\\ldots,\\kappa_K)\n",
    "  = \\frac{\\Gamma(\\kappa_0)}{\\Gamma(\\kappa_1)\\cdots\\Gamma(\\kappa_K)}\n",
    "    \\left[\\prod_{k=1}^K \\alpha_k^{\\kappa_k-1}\\right]\n",
    "    \\delta\\left(1-\\sum_{k=1}^K \\alpha_k\\right)\n",
    "$$\n",
    "where $\\kappa_0 = \\sum_{k=1}^K \\kappa_k$.\n",
    "Its normalization constant comes from the *generalized beta integral* (GBI; this is just a mathematical result that needn't refer to probability distributions):\n",
    "\\begin{split}\n",
    "\\int_0^\\infty d\\alpha_1 \\cdots \\int_0^\\infty d\\alpha_K\\;\n",
    "  & \\alpha_1^{\\kappa_1-1}\\cdots\\alpha_K^{\\kappa_K-1} \\delta\\left(a-\\sum_k\\alpha_k\\right)\\\\  % split!\n",
    "  &= \\frac{\\Gamma(\\kappa_1)\\cdots\\Gamma(\\kappa_K)}{\\Gamma(\\kappa_0)}\\; a^{\\kappa_0-1},\n",
    "\\end{split}\n",
    "where $\\kappa_0 = \\sum_{k=1}^K \\kappa_k$. Take note of the $a$ variable appearing in the GBI. To compute the Dirichlet PDF normalization, we simply set $a=1$.  But for some calculations with the Dirichlet, other choices of $a$ may be relevant. A useful special case is $K=2$, where the GBI generalizes the beta integral to cases where the two variables sum to something other than unity:\n",
    "\\begin{align}\n",
    "\\int d\\alpha_1 \\int d\\alpha_2\\;\n",
    "    \\alpha_1^{\\kappa_1-1} \\alpha_2^{\\kappa_2-1} \\delta\\left(a-[\\alpha_1+\\alpha_2]\\right)\n",
    "  &= \\int d\\alpha_1\\; \\alpha_1^{\\kappa_1-1} (a - \\alpha_1)^{\\kappa_2-1}  \\qquad \\mbox{[delta sets $\\alpha_2 = a-\\alpha_1$]}\\\\\n",
    "  &= \\frac{\\Gamma(\\kappa_1) \\Gamma(\\kappa_2)}{\\Gamma(\\kappa_1+\\kappa_2)}\\; a^{\\kappa_1+\\kappa_2-1}.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:\n",
    "\n",
    "> *Suppose you assign a symmetric Dirichlet PDF to a problem with $K=4$ categories. What are the implications of such a prior if you **aggregate** pairs of categories?*\n",
    "> * Start with the symmetric Dirichlet PDF for $K=4$, with concentration parameter $\\kappa$:\n",
    "$$\n",
    "p(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\;\\propto\\;\n",
    "  \\alpha_1^{\\kappa-1}\\alpha_2^{\\kappa-1}\\alpha_3^{\\kappa-1}\\alpha_4^{\\kappa-1}\\;\n",
    "  \\delta\\left(1-\\sum_k\\alpha_k\\right).\n",
    "$$\n",
    "Note that this drops the gamma functions comprising the normalization constant; you can (and should!) ignore parameter-independent constants throughout this problem (use the MathJax `\\propto` symbol to indicate proportionality).\n",
    "> * Consider the derived parameter $\\beta \\equiv \\alpha_1 + \\alpha_2$, the probability for an outcome being in either category 1 or category 2.  (Note that $1-\\beta = \\alpha_3 + \\alpha_4$, corresponding to aggregating the remaining two categories).  From the LTP, the prior PDF for $\\beta$ is\n",
    "\\begin{align}\n",
    "p(\\beta)\n",
    "  &= \\int d\\alpha_1 \\cdots \\int d\\alpha_4\\; p(\\beta, \\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\qquad \\mbox{[extend the conversation]}\\\\\n",
    "  &= \\int d\\alpha_1 \\cdots \\int d\\alpha_4\\; p(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\; \n",
    "     p(\\beta | \\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\qquad \\mbox{[product rule]}\\\\\n",
    "  &= \\int d\\alpha_1 \\cdots \\int d\\alpha_4\\; p(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\; \n",
    "     \\delta(\\beta - [\\alpha_1+\\alpha_2]),\n",
    "\\end{align}\n",
    "where in the last line we used\n",
    "$$\n",
    "p(\\beta | \\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) = \\delta(\\beta - [\\alpha_1+\\alpha_2]).\n",
    "$$\n",
    "That is, if we know the $\\alpha$'s, then we know that $\\beta$ must be exactly equal to $(\\alpha_1 + \\alpha_2)$, which we can enforce with a $\\delta$ function (by construction, a PDF concentrated at a point).\n",
    "\n",
    "> Your task is to do the integral giving $p(\\beta)$ and explore the result, as follows.\n",
    "\n",
    "> 1. Plug the expression for the $\\alpha$ prior into the equation for $p(\\beta)$; it will have *two* $\\delta$ functions in it, one from the $\\alpha$ prior, and one fixing $\\beta$. \n",
    "> 2. Note that the $\\beta$-dependent $\\delta$ function doesn't depend on $(\\alpha_3,\\alpha_4)$. Isolate the parts depending on $(\\alpha_3,\\alpha_4)$, and do the double integral over $\\alpha_3$ and $\\alpha_4$ using the GBI.\n",
    "> 3. Integrals over $\\alpha_1$ and $\\alpha_2$ remain.  Use the remaining $\\delta$ function to do the $\\alpha_2$ integral.\n",
    "> 4. Finally, do the $\\alpha_1$ integral using the GBI, leaving only an expression in terms of $\\beta$ and constants.\n",
    "> 5. You should find that the prior for $\\beta$ is a beta distribution (i.e., a $K=2$ Dirichlet).  What value of $\\kappa$ would make this beta distribution equal to the uniform distribution we used as a prior for inference with binomial data?  Is the original 4-category $\\alpha$ prior corresponding to this $\\kappa$ uniform with respect to $(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4)$?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
